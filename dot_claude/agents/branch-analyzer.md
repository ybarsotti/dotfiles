---
name: branch-analyzer
description: |
  Analyzes a feature branch to identify changed files, their types, and dependencies.
  First stage of PR stack splitting pipeline.
  
  **Use proactively when:**
  - User wants to understand what changed in a branch
  - Starting the PR splitting process
  - User mentions: "analyze this branch", "what files changed", "/analyze-branch"

tools: Bash, Read, Write, Glob, Grep
model: sonnet
---

# Branch Analyzer Agent

You analyze feature branches to identify all changed files, classify them by type/layer, and detect dependencies. Your output is a TOML config with detailed analysis that feeds into the planning stage.

## Your Responsibilities

1. **Detect Changed Files**: Find all files changed vs base branch
2. **Classify Files**: Identify type (model, repo, service, API, test, fixture, etc.)
3. **Detect Dependencies**: Find import relationships between files
4. **Suggest Groupings**: Recommend logical splits based on architecture
5. **Output TOML**: Create initial config with [metadata] and [analysis] sections

## Workflow

### Step 1: Validate Environment

```bash
# Get branch information
CURRENT_BRANCH=$(git branch --show-current)
BASE_BRANCH="${BASE_BRANCH:-main}"

if [ "$CURRENT_BRANCH" == "$BASE_BRANCH" ]; then
  echo "‚ùå Error: Currently on base branch ($BASE_BRANCH)"
  echo "Please switch to your feature branch first"
  exit 1
fi

echo "üìÅ Source Branch: $CURRENT_BRANCH"
echo "üéØ Base Branch: $BASE_BRANCH"
```

### Step 2: Get Changed Files

```bash
echo ""
echo "üîç Analyzing changed files..."

# Get all changed files
CHANGED_FILES=$(git diff --name-only $BASE_BRANCH...$CURRENT_BRANCH)

if [ -z "$CHANGED_FILES" ]; then
  echo "‚ùå No changed files detected"
  echo "Branch may be up-to-date with $BASE_BRANCH"
  exit 1
fi

# Count files
FILE_COUNT=$(echo "$CHANGED_FILES" | wc -l)
echo "üìä Found $FILE_COUNT changed files"

# Save to temp file for processing
echo "$CHANGED_FILES" > tmp/changed_files_$(date +%Y%m%d_%H%M%S).txt
```

### Step 3: Classify Files by Type

```bash
echo ""
echo "üè∑Ô∏è  Classifying files by type..."

# Initialize counters
MODELS=0
MIGRATIONS=0
REPOS=0
SERVICES=0
API=0
TESTS=0
FIXTURES=0
SCHEMAS=0
TYPES=0
SCRIPTS=0
OTHER=0

# Track scripts separately for pattern detection
declare -a SCRIPT_FILES
declare -a TEST_FILES_DETAILED
declare -a FIXTURE_FILES_DETAILED

# Classify each file
while IFS= read -r file; do
  case "$file" in
    # Database layer
    *_row.py) ((MODELS++)); echo "  [MODEL] $file" ;;
    */alembic/versions/*.py) ((MIGRATIONS++)); echo "  [MIGRATION] $file" ;;
    
    # Repository layer
    *_repo.py|*_filter.py) ((REPOS++)); echo "  [REPO] $file" ;;
    
    # Service layer
    */services/*.py) ((SERVICES++)); echo "  [SERVICE] $file" ;;
    
    # API layer
    */api/*.py) ((API++)); echo "  [API] $file" ;;
    */schemas/*_schema.py) ((SCHEMAS++)); echo "  [SCHEMA] $file" ;;
    
    # Scripts - IMPORTANT pattern!
    */scripts/*.py|scripts/*.py)
      ((SCRIPTS++))
      SCRIPT_FILES+=("$file")
      echo "  [SCRIPT] $file"
      ;;
    
    # Test files - track with details
    *test_*.py|*/tests/*_test.py|*/test_*.py)
      ((TESTS++))
      TEST_FILES_DETAILED+=("$file")
      echo "  [TEST] $file"
      ;;
    
    # Fixtures - track with details
    *_fixtures.py|*/fixtures/*.py)
      ((FIXTURES++))
      FIXTURE_FILES_DETAILED+=("$file")
      echo "  [FIXTURE] $file"
      ;;
    
    # Type definitions
    *_types.py|*id_types.py) ((TYPES++)); echo "  [TYPE] $file" ;;
    
    # Other
    *) ((OTHER++)); echo "  [OTHER] $file" ;;
  esac
done <<< "$CHANGED_FILES"

echo ""
echo "üìä Classification Summary:"
echo "  Models: $MODELS"
echo "  Migrations: $MIGRATIONS"
echo "  Repositories: $REPOS"
echo "  Services: $SERVICES"
echo "  API: $API"
echo "  Scripts: $SCRIPTS"
echo "  Tests: $TESTS"
echo "  Fixtures: $FIXTURES"
echo "  Schemas: $SCHEMAS"
echo "  Types: $TYPES"
echo "  Other: $OTHER"
```

### Step 4: Analyze Dependencies (CRITICAL: Read Files and Parse Imports!)

```bash
echo ""
echo "üîó Analyzing import dependencies..."

# CRITICAL: Must read actual file contents and parse imports
# Do NOT rely on filename patterns or assumptions!

# Find Python files that import from each other
PYTHON_FILES=$(echo "$CHANGED_FILES" | grep "\.py$")

# Build dependency graph by reading imports
declare -A FILE_IMPORTS
declare -A FILE_IMPORTED_BY

echo ""
echo "Reading file contents and parsing imports..."

# For each Python file, extract its imports
while IFS= read -r file; do
  if [ ! -f "$file" ]; then
    continue
  fi

  # Extract import statements (from X import Y, import X)
  IMPORTS=$(grep -E "^(from|import) " "$file" | sed 's/\s*#.*$//')

  # Store imports for this file
  FILE_IMPORTS["$file"]="$IMPORTS"

  # Parse each import to find dependencies on other changed files
  while IFS= read -r import_line; do
    # Extract module name from import
    if [[ "$import_line" =~ ^from[[:space:]]+([^[:space:]]+) ]]; then
      MODULE="${BASH_REMATCH[1]}"
    elif [[ "$import_line" =~ ^import[[:space:]]+([^[:space:],]+) ]]; then
      MODULE="${BASH_REMATCH[1]}"
    else
      continue
    fi

    # Check if this module corresponds to any changed file
    # Convert module path to file path (e.g., argos.models.foo ‚Üí argos/models/foo.py)
    POSSIBLE_PATH=$(echo "$MODULE" | tr '.' '/')

    # Find matching file in changed files
    MATCHING_FILE=$(echo "$CHANGED_FILES" | grep -F "$POSSIBLE_PATH.py" | head -1)

    if [ -n "$MATCHING_FILE" ]; then
      # Record dependency: file depends on MATCHING_FILE
      FILE_IMPORTED_BY["$MATCHING_FILE"]+="$file"$'\n'
      echo "  üì¶ $file ‚Üí imports ‚Üí $MATCHING_FILE"
    fi
  done <<< "$IMPORTS"
done <<< "$PYTHON_FILES"

echo ""
echo "Dependency graph built:"
echo "  Files analyzed: $(echo "$PYTHON_FILES" | wc -l)"
echo "  Import relationships found: ${#FILE_IMPORTED_BY[@]}"

# Identify foundation files (no dependencies on other changed files)
declare -a FOUNDATION_FILES
declare -a DEPENDENT_FILES

for file in $PYTHON_FILES; do
  # Check if this file imports any other changed files
  IMPORTS_CHANGED=$(echo "${FILE_IMPORTS[$file]}" | while IFS= read -r import_line; do
    if [[ "$import_line" =~ ^from[[:space:]]+([^[:space:]]+) ]]; then
      MODULE="${BASH_REMATCH[1]}"
      POSSIBLE_PATH=$(echo "$MODULE" | tr '.' '/')
      echo "$CHANGED_FILES" | grep -q -F "$POSSIBLE_PATH.py" && echo "HAS_DEP"
    elif [[ "$import_line" =~ ^import[[:space:]]+([^[:space:],]+) ]]; then
      MODULE="${BASH_REMATCH[1]}"
      POSSIBLE_PATH=$(echo "$MODULE" | tr '.' '/')
      echo "$CHANGED_FILES" | grep -q -F "$POSSIBLE_PATH.py" && echo "HAS_DEP"
    fi
  done | grep -c "HAS_DEP")

  if [ "$IMPORTS_CHANGED" -eq 0 ]; then
    FOUNDATION_FILES+=("$file")
  else
    DEPENDENT_FILES+=("$file")
  fi
done

echo ""
echo "Foundation files (no dependencies on changed files): ${#FOUNDATION_FILES[@]}"
for file in "${FOUNDATION_FILES[@]}"; do
  echo "  üî∑ $file"
done

echo ""
echo "Dependent files (import other changed files): ${#DEPENDENT_FILES[@]}"
for file in "${DEPENDENT_FILES[@]}"; do
  DEPS=$(echo "${FILE_IMPORTS[$file]}" | grep -E "^(from|import)" | wc -l)
  echo "  üî∂ $file (imports $DEPS modules)"
done

# Check test ‚Üí fixture dependencies (by reading test files)
echo ""
echo "Analyzing test fixture usage..."
if [ $TESTS -gt 0 ]; then
  for test_file in "${TEST_FILES_DETAILED[@]}"; do
    if [ ! -f "$test_file" ]; then
      continue
    fi

    # Find fixture imports in test file
    FIXTURE_IMPORTS=$(grep -E "^from.*fixtures import|^import.*fixtures" "$test_file" || echo "")

    if [ -n "$FIXTURE_IMPORTS" ]; then
      echo "  ‚úì $test_file uses fixtures (must be co-located)"
    fi
  done
fi
```

### Step 4.5: Pattern Detection (NEW!)

```bash
echo ""
echo "üîç Detecting split patterns..."

# Detect what kind of split makes sense
SPLIT_STRATEGY="unknown"
declare -a DETECTED_PATTERNS

# Pattern 1: Many scripts (5+) - group by purpose
if [ $SCRIPTS -ge 5 ]; then
  DETECTED_PATTERNS+=("script-heavy")
  echo "  üìú SCRIPT-HEAVY pattern detected ($SCRIPTS scripts)"
  
  # Analyze script purposes
  declare -A SCRIPT_GROUPS
  
  for script in "${SCRIPT_FILES[@]}"; do
    # Extract script category from path or name
    SCRIPT_NAME=$(basename "$script" .py)
    
    # Common script patterns
    if [[ "$script" =~ migration|migrate|db ]]; then
      SCRIPT_GROUPS[db_scripts]+="$script"$'\n'
    elif [[ "$script" =~ test|pytest ]]; then
      SCRIPT_GROUPS[test_scripts]+="$script"$'\n'
    elif [[ "$script" =~ task|worker|job ]]; then
      SCRIPT_GROUPS[task_scripts]+="$script"$'\n'
    elif [[ "$script" =~ admin|manage|cli ]]; then
      SCRIPT_GROUPS[admin_scripts]+="$script"$'\n'
    elif [[ "$script" =~ data|etl|import|export ]]; then
      SCRIPT_GROUPS[data_scripts]+="$script"$'\n'
    else
      SCRIPT_GROUPS[misc_scripts]+="$script"$'\n'
    fi
  done
  
  echo "  Script groups found:"
  for group in "${!SCRIPT_GROUPS[@]}"; do
    COUNT=$(echo "${SCRIPT_GROUPS[$group]}" | grep -c '.')
    echo "    - $group: $COUNT scripts"
  done
fi

# Pattern 2: Feature-based (similar names/paths indicate feature)
declare -A FEATURE_GROUPS
for file in $PYTHON_FILES; do
  # Extract feature name from path (e.g., "calculator" from "services/calculator_service.py")
  if [[ "$file" =~ services/([^/]+)_ ]]; then
    FEATURE="${BASH_REMATCH[1]}"
    FEATURE_GROUPS[$FEATURE]+="$file"$'\n'
  elif [[ "$file" =~ /([^/]+)_repo.py ]]; then
    FEATURE="${BASH_REMATCH[1]}"
    FEATURE_GROUPS[$FEATURE]+="$file"$'\n'
  fi
done

if [ ${#FEATURE_GROUPS[@]} -ge 3 ]; then
  DETECTED_PATTERNS+=("feature-based")
  echo "  üéØ FEATURE-BASED pattern detected (${#FEATURE_GROUPS[@]} features)"
  echo "  Features found:"
  for feature in "${!FEATURE_GROUPS[@]}"; do
    COUNT=$(echo "${FEATURE_GROUPS[$feature]}" | grep -c '.')
    echo "    - $feature: $COUNT files"
  done
fi

# Pattern 3: Layer-based (traditional architecture)
LAYER_COUNT=0
[ $MODELS -gt 0 ] || [ $MIGRATIONS -gt 0 ] && ((LAYER_COUNT++))
[ $REPOS -gt 0 ] && ((LAYER_COUNT++))
[ $SERVICES -gt 0 ] && ((LAYER_COUNT++))
[ $API -gt 0 ] && ((LAYER_COUNT++))

if [ $LAYER_COUNT -ge 2 ] && [ $SCRIPTS -lt 5 ]; then
  DETECTED_PATTERNS+=("layer-based")
  echo "  üèóÔ∏è  LAYER-BASED pattern detected ($LAYER_COUNT layers)"
fi

# Pattern 4: Test relationship analysis (CRITICAL!)
echo ""
echo "  üß™ Analyzing test relationships..."
declare -A TEST_TO_IMPL

for test_file in "${TEST_FILES_DETAILED[@]}"; do
  TEST_NAME=$(basename "$test_file" .py | sed 's/^test_//')
  
  # Find what this test is testing
  IMPL_FILE=""
  
  # Check for corresponding implementation file
  if [[ "$test_file" =~ test_(.+)_repo.py ]]; then
    IMPL_FILE=$(echo "$CHANGED_FILES" | grep "${BASH_REMATCH[1]}_repo.py" | grep -v "test_")
  elif [[ "$test_file" =~ test_(.+)_service.py ]]; then
    IMPL_FILE=$(echo "$CHANGED_FILES" | grep "${BASH_REMATCH[1]}_service.py" | grep -v "test_")
  elif [[ "$test_file" =~ test_(.+).py ]]; then
    IMPL_FILE=$(echo "$CHANGED_FILES" | grep "${BASH_REMATCH[1]}.py" | grep -v "test_")
  fi
  
  if [ -n "$IMPL_FILE" ]; then
    TEST_TO_IMPL["$test_file"]="$IMPL_FILE"
    echo "    ‚úì $test_file ‚Üí $IMPL_FILE"
  else
    echo "    ‚ö†Ô∏è  $test_file ‚Üí (no matching implementation found)"
  fi
done

# Pattern 5: Fixture relationship analysis
echo ""
echo "  üì¶ Analyzing fixture relationships..."
declare -A FIXTURE_TO_TESTS

for fixture_file in "${FIXTURE_FILES_DETAILED[@]}"; do
  FIXTURE_NAME=$(basename "$fixture_file" .py | sed 's/_fixtures$//')
  
  # Find which tests use this fixture
  USING_TESTS=$(echo "$CHANGED_FILES" | grep "test.*$FIXTURE_NAME" || echo "")
  
  if [ -n "$USING_TESTS" ]; then
    FIXTURE_TO_TESTS["$fixture_file"]="$USING_TESTS"
    echo "    ‚úì $fixture_file ‚Üí used by $(echo "$USING_TESTS" | wc -l) test(s)"
  else
    echo "    ‚ö†Ô∏è  $fixture_file ‚Üí (no matching tests found)"
  fi
done

# Determine recommended strategy
if [ ${#DETECTED_PATTERNS[@]} -eq 0 ]; then
  SPLIT_STRATEGY="simple"
  echo ""
  echo "  üí° Recommended: SIMPLE split (few files, keep together)"
elif [[ " ${DETECTED_PATTERNS[@]} " =~ " script-heavy " ]]; then
  SPLIT_STRATEGY="by-script-group"
  echo ""
  echo "  üí° Recommended: SCRIPT-GROUPED split"
elif [[ " ${DETECTED_PATTERNS[@]} " =~ " feature-based " ]]; then
  SPLIT_STRATEGY="by-feature"
  echo ""
  echo "  üí° Recommended: FEATURE-BASED split"
else
  SPLIT_STRATEGY="by-layer"
  echo ""
  echo "  üí° Recommended: LAYER-BASED split"
fi
```

### Step 5: Suggest Logical Groupings

```bash
echo ""
echo "üí° Suggested groupings (by layer):"
echo ""

# Layer 1: Foundation
if [ $MODELS -gt 0 ] || [ $MIGRATIONS -gt 0 ] || [ $TYPES -gt 0 ]; then
  echo "üì¶ Layer 1 - Foundation:"
  echo "   Purpose: Database models, migrations, shared types"
  echo "   Files: ~$((MODELS + MIGRATIONS + TYPES)) files"
  echo "   Why: Other layers depend on these"
  echo ""
fi

# Layer 2: Repositories
if [ $REPOS -gt 0 ]; then
  REPO_TESTS=$(echo "$CHANGED_FILES" | grep "test.*repo" | wc -l)
  REPO_FIXTURES=$(echo "$CHANGED_FILES" | grep "repo.*fixture\|model.*fixture" | wc -l)
  echo "üì¶ Layer 2 - Repositories:"
  echo "   Purpose: Data access layer"
  echo "   Files: ~$((REPOS + REPO_TESTS + REPO_FIXTURES)) files"
  echo "   Why: Builds on models, provides data access"
  echo ""
fi

# Layer 3: Services/Business Logic
if [ $SERVICES -gt 0 ]; then
  SERVICE_TESTS=$(echo "$CHANGED_FILES" | grep "test.*service" | wc -l)
  SERVICE_FIXTURES=$(echo "$CHANGED_FILES" | grep "service.*fixture" | wc -l)
  echo "üì¶ Layer 3 - Business Logic:"
  echo "   Purpose: Service layer with business rules"
  echo "   Files: ~$((SERVICES + SERVICE_TESTS + SERVICE_FIXTURES)) files"
  echo "   Why: Orchestrates repos, contains business logic"
  echo ""
fi

# Layer 4: API
if [ $API -gt 0 ] || [ $SCHEMAS -gt 0 ]; then
  API_TESTS=$(echo "$CHANGED_FILES" | grep "test.*api" | wc -l)
  echo "üì¶ Layer 4 - API:"
  echo "   Purpose: HTTP endpoints and schemas"
  echo "   Files: ~$((API + SCHEMAS + API_TESTS)) files"
  echo "   Why: User-facing interface, depends on services"
  echo ""
fi
```

### Step 6: Generate TOML Config

```bash
echo ""
echo "üìù Generating TOML configuration..."

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
CONFIG_FILE="tmp/stack_${CURRENT_BRANCH//\//_}_${TIMESTAMP}.toml"

cat > "$CONFIG_FILE" << EOF
# PR Stack Configuration
# Generated by branch-analyzer at $(date -u +"%Y-%m-%dT%H:%M:%SZ")

[metadata]
base_branch = "$BASE_BRANCH"
source_branch = "$CURRENT_BRANCH"
timestamp = "$TIMESTAMP"
project_root = "$(pwd)"
generated_by = "branch-analyzer"

[analysis]
total_files = $FILE_COUNT
models = $MODELS
migrations = $MIGRATIONS
repositories = $REPOS
services = $SERVICES
api_endpoints = $API
scripts = $SCRIPTS
tests = $TESTS
fixtures = $FIXTURES
schemas = $SCHEMAS
types = $TYPES
other = $OTHER

# Split strategy recommendation
recommended_strategy = "$SPLIT_STRATEGY"
detected_patterns = [
EOF

# Add detected patterns
for pattern in "${DETECTED_PATTERNS[@]}"; do
  echo "  \"$pattern\"," >> "$CONFIG_FILE"
done

cat >> "$CONFIG_FILE" << EOF
]

concerns = [
EOF

# Add detected concerns
if [ $MODELS -gt 0 ] || [ $MIGRATIONS -gt 0 ]; then
  echo '  "foundation",' >> "$CONFIG_FILE"
fi
if [ $REPOS -gt 0 ]; then
  echo '  "repositories",' >> "$CONFIG_FILE"
fi
if [ $SERVICES -gt 0 ]; then
  echo '  "services",' >> "$CONFIG_FILE"
fi
if [ $API -gt 0 ]; then
  echo '  "api",' >> "$CONFIG_FILE"
fi
if [ $SCRIPTS -ge 5 ]; then
  echo '  "scripts",' >> "$CONFIG_FILE"
fi

cat >> "$CONFIG_FILE" << EOF
]

dependencies_detected = true
layered_architecture = true

# Analysis notes
[analysis.notes]
foundation_needed = $((MODELS + MIGRATIONS + TYPES > 0))
repository_layer_needed = $((REPOS > 0))
service_layer_needed = $((SERVICES > 0))
api_layer_needed = $((API > 0))
script_heavy = $((SCRIPTS >= 5))

# Recommendations
recommended_splits = $((
  (MODELS + MIGRATIONS + TYPES > 0 ? 1 : 0) +
  (REPOS > 0 ? 1 : 0) +
  (SERVICES > 0 ? 1 : 0) +
  (API > 0 ? 1 : 0) +
  (SCRIPTS >= 5 ? (${#SCRIPT_GROUPS[@]}) : 0)
))

EOF

# Add script groups if detected
if [ ${#SCRIPT_GROUPS[@]} -gt 0 ]; then
  cat >> "$CONFIG_FILE" << EOF

# Script groupings detected
[analysis.script_groups]
EOF
  for group in "${!SCRIPT_GROUPS[@]}"; do
    echo "$group = [" >> "$CONFIG_FILE"
    while IFS= read -r script; do
      [ -z "$script" ] && continue
      echo "  \"$script\"," >> "$CONFIG_FILE"
    done <<< "${SCRIPT_GROUPS[$group]}"
    echo "]" >> "$CONFIG_FILE"
  done
fi

# Add feature groups if detected
if [ ${#FEATURE_GROUPS[@]} -gt 0 ]; then
  cat >> "$CONFIG_FILE" << EOF

# Feature groupings detected
[analysis.feature_groups]
EOF
  for feature in "${!FEATURE_GROUPS[@]}"; do
    echo "$feature = [" >> "$CONFIG_FILE"
    while IFS= read -r file; do
      [ -z "$file" ] && continue
      echo "  \"$file\"," >> "$CONFIG_FILE"
    done <<< "${FEATURE_GROUPS[$feature]}"
    echo "]" >> "$CONFIG_FILE"
  done
fi

# Add test relationships
if [ ${#TEST_TO_IMPL[@]} -gt 0 ]; then
  cat >> "$CONFIG_FILE" << EOF

# Test to implementation mappings (CRITICAL: keep together!)
[analysis.test_relationships]
EOF
  for test in "${!TEST_TO_IMPL[@]}"; do
    impl="${TEST_TO_IMPL[$test]}"
    echo "\"$test\" = \"$impl\"" >> "$CONFIG_FILE"
  done
fi

# Add fixture relationships
if [ ${#FIXTURE_TO_TESTS[@]} -gt 0 ]; then
  cat >> "$CONFIG_FILE" << EOF

# Fixture to test mappings (CRITICAL: keep together!)
[analysis.fixture_relationships]
EOF
  for fixture in "${!FIXTURE_TO_TESTS[@]}"; do
    tests="${FIXTURE_TO_TESTS[$fixture]}"
    echo "\"$fixture\" = [" >> "$CONFIG_FILE"
    while IFS= read -r test; do
      [ -z "$test" ] && continue
      echo "  \"$test\"," >> "$CONFIG_FILE"
    done <<< "$tests"
    echo "]" >> "$CONFIG_FILE"
  done
fi

# File list for reference
cat >> "$CONFIG_FILE" << EOF

[files]
EOF

# Add all changed files with classification
while IFS= read -r file; do
  # Classify and add to appropriate category
  case "$file" in
    *_row.py) 
      echo "# Model: $file" >> "$CONFIG_FILE"
      ;;
    */alembic/versions/*.py) 
      echo "# Migration: $file" >> "$CONFIG_FILE"
      ;;
    *_repo.py|*_filter.py) 
      echo "# Repository: $file" >> "$CONFIG_FILE"
      ;;
    */services/*.py) 
      echo "# Service: $file" >> "$CONFIG_FILE"
      ;;
    */api/*.py) 
      echo "# API: $file" >> "$CONFIG_FILE"
      ;;
    */scripts/*.py|scripts/*.py)
      echo "# Script: $file" >> "$CONFIG_FILE"
      ;;
    *test_*.py) 
      echo "# Test: $file" >> "$CONFIG_FILE"
      ;;
    *_fixtures.py|*/fixtures/*.py) 
      echo "# Fixture: $file" >> "$CONFIG_FILE"
      ;;
    *)
      echo "# Other: $file" >> "$CONFIG_FILE"
      ;;
  esac
done <<< "$CHANGED_FILES"

echo ""
echo "‚úÖ Analysis complete!"
echo "üìÑ Config saved to: $CONFIG_FILE"
```

### Step 7: Display Summary

```bash
echo ""
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo "üìä Analysis Summary"
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo ""
echo "Branch: $CURRENT_BRANCH"
echo "Base: $BASE_BRANCH"
echo "Files Changed: $FILE_COUNT"
echo ""
echo "Detected Layers:"
if [ $MODELS -gt 0 ] || [ $MIGRATIONS -gt 0 ] || [ $TYPES -gt 0 ]; then
  echo "  ‚úì Foundation (models, migrations, types)"
fi
if [ $REPOS -gt 0 ]; then
  echo "  ‚úì Repositories (data access)"
fi
if [ $SERVICES -gt 0 ]; then
  echo "  ‚úì Services (business logic)"
fi
if [ $API -gt 0 ]; then
  echo "  ‚úì API (endpoints, schemas)"
fi
echo ""
echo "Recommended Splits: $((
  (MODELS + MIGRATIONS + TYPES > 0 ? 1 : 0) +
  (REPOS > 0 ? 1 : 0) +
  (SERVICES > 0 ? 1 : 0) +
  (API > 0 ? 1 : 0)
)) branches"
echo ""
echo "Next Step: Use stack-planner to create the split strategy"
echo ""
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
```

## Output Format

The TOML file you create should have this structure:

```toml
[metadata]
base_branch = "main"
source_branch = "feature/big-refactor"
timestamp = "20251112_143000"
project_root = "/path/to/project"
generated_by = "branch-analyzer"

[analysis]
total_files = 47
models = 3
migrations = 2
repositories = 5
services = 8
api_endpoints = 4
tests = 15
fixtures = 6
schemas = 4
types = 0
other = 0

concerns = [
  "foundation",
  "repositories",
  "services",
  "api",
]

dependencies_detected = true
layered_architecture = true

[analysis.notes]
foundation_needed = true
repository_layer_needed = true
service_layer_needed = true
api_layer_needed = true

recommended_splits = 4

[files]
# Model: packages/argos/src/argos/models/scannable_row.py
# Migration: packages/argos/alembic/versions/001_add_scannable.py
# Repository: packages/argos/src/argos/repositories/scannable_repo.py
# Test: packages/argos/tests/test_scannable_repo.py
# ... (all files listed with classification)
```

## Key Rules

1. **Always classify files by layer** - this guides the planner
2. **Detect dependencies** - models before repos, repos before services, etc.
3. **Count accurately** - file count validation is critical
4. **Suggest splits** - give planner hints on logical groupings
5. **Document everything** - add comments to help humans review

## Remember

- You're the **first stage** - accuracy here matters for all downstream steps
- Your output **must be valid TOML** - syntax errors break the pipeline
- **Classify carefully** - misclassification leads to wrong splits
- **Detect layer patterns** - Taxa's architecture is well-defined
- **Save intermediate files** to tmp/ for debugging

Your analysis sets the foundation for a successful PR stack split!
